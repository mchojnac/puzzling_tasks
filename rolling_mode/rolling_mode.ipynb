{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of my projects I have come across such task. Find the most common value of in string column in 30 minute windows, when we have one measurement per second. The length of data frame can be from 1h to any other bigger size.\n",
    "One more thing was that the number of possible strings in the column is <20.\n",
    "\n",
    "The propose solutions was based on pandas rolling and mode function from scipy. It was very slow, I have come with much faster solution which I want to show and compare with the rolling one in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a data to show a problem\n",
    "# I will create a data frame with  a random string column\n",
    "# I will not evenly distributed strings in this column, it will be clear later why \n",
    "table1 = [\"A\", \"B\", \"C\", \"D\"]\n",
    "table2 = [\"V\", \"X\", \"Y\", \"Z\"]\n",
    "l_input = [table1[np.random.randint(0, 4)] for _ in range(5*3600)]+ \\\n",
    "          [table2[np.random.randint(0, 4)] for _ in range(5*3600)]\n",
    "df_test = pd.DataFrame(data={\"test\": l_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task is to calculate the most common value in windows of size 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RollingWay(df,column_name=\"test\",window_size=1800):\n",
    "    # first we have to change string to int beacuse rolling can not work with string columns\n",
    "    # so we take all unique string and create dict for mapping\n",
    "    all_str = np.sort(df[column_name].unique()) # sorting is to have predictable mapping \n",
    "    map_dict = {i: ix for ix, i in enumerate(all_str)}\n",
    "    map_column_name =\"mapped_\" +column_name\n",
    "    df[map_column_name] = df[column_name].map(map_dict)\n",
    "    # finall rolling\n",
    "    df[\"results\"] = df[map_column_name].rolling(window=window_size, min_periods=window_size, center=True).\\\n",
    "                                        apply(lambda x: (mode(x)[0]), raw=True)\n",
    "    return df[\"results\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.502422888\n"
     ]
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "v1=RollingWay(df_test)\n",
    "elapsed_time = time.process_time() - t\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the biggest problem of the rolling approach is that step n does not use any info from step n-1.\n",
    "The difference between those two steps are is in two values.\n",
    "I have coded function using that to check how faster it will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyWay(df,column_name=\"test\",window_size=1800):\n",
    "    # as first we also have to get all unique values\n",
    "    all_str = np.sort(df[column_name].unique())\n",
    "    # the we get results for the first window\n",
    "    data_dict = df[column_name][: window_size].value_counts().to_dict()\n",
    "    # creates a dictionary for the first window which will include all strings in column \n",
    "    # this is why evenly distributed strings where used\n",
    "    data_dict.update({i:0 for i in all_str if i not in data_dict.keys()})\n",
    "    # get the list which will store results\n",
    "    results = list()\n",
    "    # put results for first window \n",
    "    results.append(max(data_dict, key=data_dict.get))\n",
    "     #in each step we removing count for the low index and add count from the up index \n",
    "    for iplus, iminus in zip(df[column_name][window_size:].values, df[column_name][:-1*window_size].values):\n",
    "        data_dict[iplus] = data_dict[iplus]+1\n",
    "        data_dict[iminus] = data_dict[iminus]-1\n",
    "        # add we get results by finding the most common value\n",
    "        results.append(max(data_dict, key=data_dict.get))\n",
    "    return np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06876973699999933\n"
     ]
    }
   ],
   "source": [
    "t = time.process_time()\n",
    "v2=MyWay(df_test)\n",
    "elapsed_time = time.process_time() - t\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My way is much faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
